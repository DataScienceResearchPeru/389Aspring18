{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Introduction\n",
    "\n",
    "**Course:** CMSC 389A - Practical Deep Learning \n",
    "\n",
    "**Author:** Sujith Vishwajith   \n",
    "\n",
    "**Task:** Up until now we have been focusing on classification based tasks and fully supervised learning techniques (given a label for every example). We will now discuss a new framework called Reinforcement Learning (RL) which is in between supervised and unsupervised learning. The goal of RL is to learn how to make an agent take actions at each step (decision making) given an environment with nothing but a reward signal.  \n",
    "\n",
    "An example of this is learning how to play the game Pong where the environment is the game and the agent which you control is the paddle. A sample signal for this example could be a negative reward if the ball goes in your goal, and a positive reward if you score. The goal is to maximize the reward and as a result learn how to play the game optimally.\n",
    "\n",
    "This notebook is meant to supplement this weeks lecture on Reinforcement Learning.\n",
    " \n",
    "**Packages**  \n",
    "Lets import the following required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, we will play the game N-Chain available on OpenAI's gym platform. More informationa bout N-Chain can be found here: https://gym.openai.com/envs/NChain-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('NChain-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample play to get an idea of how it works. env.step() returns a tuple: (new state, reward received, boolean if game ended, extraneous debugging info). Action 0 means move forward and 1 means move back to the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2, False, {})\n",
      "(1, 0, False, {})\n",
      "(0, 2, False, {})\n",
      "(0, 2, False, {})\n",
      "(1, 0, False, {})\n",
      "(0, 2, False, {})\n",
      "(1, 0, False, {})\n",
      "(0, 2, False, {})\n",
      "(0, 2, False, {})\n",
      "(0, 2, False, {})\n"
     ]
    }
   ],
   "source": [
    "print(env.step(1))\n",
    "print(env.step(0))\n",
    "print(env.step(1))\n",
    "print(env.step(0))\n",
    "print(env.step(0))\n",
    "print(env.step(1))\n",
    "print(env.step(1))\n",
    "print(env.step(1))\n",
    "print(env.step(1))\n",
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(table, env):\n",
    "    s = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        reward += r\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_games(env, num_iterations=100):\n",
    "    winner = [0,0,0]\n",
    "    for i in range(num_iterations):\n",
    "        m0_table = naive_sum(env, 500)\n",
    "        m0 = play(m0_table, env)\n",
    "        m1_table = q_learning_basic(env, 500)\n",
    "        m1 = play(m1_table, env)\n",
    "        m2_table = q_learning_exploration(env, 500)\n",
    "        m2 = play(m2_table, env)\n",
    "        results = np.array([m0, m1, m2])\n",
    "        w = np.argmax(results)\n",
    "        winner[w] += 1\n",
    "        print(\"Game {:} of {:} -> {:}\".format(i + 1, num_iterations, results))\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sum(env, num_episodes=500):\n",
    "    table = np.zeros((5, 2))\n",
    "    for episode in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            table[s, a] += r\n",
    "            s = new_s\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 98532.,      0.],\n",
       "       [     0., 315794.],\n",
       "       [     0.,  63150.],\n",
       "       [     0.,  12772.],\n",
       "       [ 66664.,      0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_table = naive_sum(env, 500)\n",
    "naive_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1006\n"
     ]
    }
   ],
   "source": [
    "score = play(naive_table, env)\n",
    "print('Score: {:}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_basic(env, num_episodes=500):\n",
    "    table = np.zeros((5, 2))\n",
    "    # Discount factor\n",
    "    discount = 0.95\n",
    "    # Learning Rate\n",
    "    alpha = 0.8\n",
    "    for episode in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(table[s,:]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            table[s, a] += r + alpha*(discount*np.max(table[new_s, :]) - table[s, a])\n",
    "            s = new_s\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 27.97472762],\n",
       "       [26.60945604,  0.        ],\n",
       "       [ 0.        , 27.60546352],\n",
       "       [38.1436722 ,  0.        ],\n",
       "       [44.00757334,  0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_basic_table = q_learning_basic(env, 500)\n",
    "q_basic_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2052\n"
     ]
    }
   ],
   "source": [
    "score = play(q_basic_table, env)\n",
    "print('Score: {:}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning + Greedy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_exploration(env, num_episodes=500):\n",
    "    table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.random() < eps or np.sum(table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            table[s, a] += r + lr * (y * np.max(table[new_s, :]) - table[s, a])\n",
    "            s = new_s\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[79.97423962, 78.71168957],\n",
       "       [78.17795861, 61.34546629],\n",
       "       [72.26927274, 66.21014379],\n",
       "       [68.40601872, 42.40273781],\n",
       "       [79.70875469, 54.02538677]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_explore_table = q_learning_exploration(env, 500)\n",
    "q_explore_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 3642\n"
     ]
    }
   ],
   "source": [
    "score = play(q_explore_table, env)\n",
    "print('Score: {:}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 of 5 -> [1584 1576 1446]\n",
      "Game 2 of 5 -> [1542 1394  850]\n",
      "Game 3 of 5 -> [1632 1288 2868]\n",
      "Game 4 of 5 -> [1606 1616 1666]\n",
      "Game 5 of 5 -> [1612  966 3872]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 0, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_games(env, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_keras(env, num_episodes=1000):\n",
    "    # Building the Model\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    # Parameters\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    decay_factor = 0.999\n",
    "\n",
    "    # Q-Learning Part\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        if i % 100 == 0:\n",
    "            print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.random() < eps:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "            target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "            target_vec[a] = target\n",
    "            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "            s = new_s\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
