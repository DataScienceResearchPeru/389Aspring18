{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First_Name Last_Name\n",
    "\n",
    "Please replace first_name and last_name with your information. This notebook will be your template that you should follow for this project. Feel free to create any subsections within each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Loading the data\n",
    "\n",
    "There is no code for you to fill out in this section but please make sure you understand what the code* is doing so you aren't confused in later parts. Basically, this section loads the training, testing, and validation data of the dog images for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, Lambda, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image    \n",
    "from matplotlib import pyplot\n",
    "from scipy.misc import toimage\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the datasets and gets list of dog names (labels). `X_train`, `X_val`, and `X_test` contain lists of image paths to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "X_train, y_train = load_dataset('dogImages/train')\n",
    "X_val, y_val = load_dataset('dogImages/valid')\n",
    "X_test, y_test = load_dataset('dogImages/test')\n",
    "\n",
    "# List of all dog names\n",
    "dog_labels = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_image` takes in a string containing a path to an image, loads the image file, converts it into a array matrix, resizes it go be `64x64`, and normalizes all the values to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size=32):\n",
    "    img = Image.open(path.strip())\n",
    "    img = np.array(img, np.float32)\n",
    "    img = scipy.misc.imresize(img, (size, size))\n",
    "    img = np.divide(img, 255.0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(load_image(X_train[i]))\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data augmentation\n",
    "\n",
    "In this section we want you to perform some sort of data augmentation as we learnt about in class. This is because we only have roughly 200 examples per label which in deep learning isn't generally enough. Therefore, to improve performance we can use data augmentation as a trick to generate more training examples.\n",
    "\n",
    "Make sure you try at least **TWO** methods of augmentation. Please explain each augmentation and what it does as well as visualizing at least one example of each augmentation. If you do choose to try more, make sure to incremement `NB_AUGMENTATIONS` by one each time.\n",
    "\n",
    "If you want to change the size of your image (smaller images train faster), you want to change `IMG_SIZE`.\n",
    "\n",
    "An example augmentation of flipping the image has already been done for you in `example_augment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_augment(img):\n",
    "    augmented_img = np.fliplr(img)\n",
    "    return augmented_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentation Explanation:** FOR YOU TO FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_1(img):\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    augmented_img = img\n",
    "    \n",
    "    return augmented_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentation Explanation:** FOR YOU TO FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_2(img):\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    augmented_img = img\n",
    "    \n",
    "    return augmented_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_AUGMENTATIONS = 1\n",
    "IMG_SIZE = 32\n",
    "\n",
    "def batch_generator(images, labels, augment=True, batch_size=32):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle indices to minimize overfitting.\n",
    "        for i in np.random.permutation(images.shape[0]):\n",
    "\n",
    "            img_path = images[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            img = load_image(img_path, size=IMG_SIZE)\n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(label)\n",
    "            sample_count += 1\n",
    "\n",
    "            if augment:\n",
    "                # Perform Augmentation (Flipping the Image)\n",
    "                img_example_augment = example_augment(img)\n",
    "                batch_images.append(img_example_augment)\n",
    "                batch_labels.append(label)\n",
    "                \n",
    "                # Perform Augmentation 1\n",
    "                img_augment_1 = augmentation_1(img)\n",
    "                batch_images.append(img_augment_1)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "                # Perform Augmentation 2\n",
    "                img_augment_2 = augmentation_2(img)\n",
    "                batch_images.append(img_augment_2)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "            if ((sample_count % batch_size == 0) or (sample_count % len(images) == 0)):\n",
    "                yield np.array(batch_images), np.array(batch_labels)\n",
    "                # Reset batch.\n",
    "                batch_images = []\n",
    "                batch_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Building the model\n",
    "\n",
    "In this section you will write all the code to build your Convolutional Neural Network model in Keras. The model should output `NUM_CLASSES` (133 in this case) values which sum up to 1 and are each the probability that the dog is of the respective breed.\n",
    "\n",
    "**Explanation:** FOR YOU TO FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 127.5 - 1.0, input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "\n",
    "# FOR YOU TO BUILD YOUR MODEL\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR YOU TO CHOOSE\n",
    "optimizer = None\n",
    "loss = None\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Training the model\n",
    "\n",
    "The code to train your model has been done for you already as it is a bit different than before due to the batch generator. Notice how we set `augment` to false for the validation generator. It is important never to augment your validation nor testing data since that can bias (inflate or deflate) the accuracy scores. The code also saves your models weights in the end which we can use for testing later on so we don't have to retrain it entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train = batch_generator(X_train, y_train, augment=True)\n",
    "generator_val = batch_generator(X_val, y_val, augment=False)\n",
    "\n",
    "model.fit_generator(\n",
    "        generator_train,\n",
    "        steps_per_epoch= (NB_AUGMENTATIONS+1) * len(X_train),\n",
    "        epochs=10,\n",
    "        validation_data=generator_val,\n",
    "        validation_steps=len(X_val),\n",
    "        verbose=1)\n",
    "\n",
    "# Save the weights for grading.\n",
    "model.save_weights('model.h5', True)\n",
    "with open('model.json', 'w') as outfile:\n",
    "    json.dump(model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will compute the accuracy of your model on the test data from Section 0. Make sure you very clearly have a cell that outputs and prints the percentage accuracy of your model. You will also include any code used to analyze the results here.\n",
    "\n",
    "A confusion matrix would be nice but since there are so many classes it will be hard to cleanly visualize. Instead f1, precision, and recall scores for each class would be cleaner to read.\n",
    "\n",
    "**Explanation:** FOR YOU TO FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Project Code inspired and taken from Udacity's AI Nanodegress https://github.com/mahavird/dog-project. All credit for the helper methods to load the dataset goes to Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
